{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On NLP\n",
    "## Project : Text Generation and Chatbot for Song Lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:magenta\">Group Names:</span>\n",
    "\n",
    "* Petko Petkov\n",
    "* Manda ANDRIAMAROMANANA\n",
    "* Ilyes SAIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Generate descriptions  (or the complete instructions that we're going to use for the training of our model) for the song lyrics in the dataset, we should use another more capable model like ChatGPT, Claude, Gemini, etc.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from transformers import pipeline\n",
    "\n",
    "INPUT_DATASET_REPO = \"vishnupriyavr/spotify-million-song-dataset\"\n",
    "OUTPUT_DATASET_REPO = \"petkopetkov/spotify-million-song-dataset-descriptions\"\n",
    "HF_DATASET_SPLIT = \"train\"\n",
    "CHECKPOINT_FILE = \"songs_descriptions_checkpoint.json\"\n",
    "\n",
    "dataset = load_dataset(INPUT_DATASET_REPO, split=HF_DATASET_SPLIT)\n",
    "\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "        checkpoint_data = json.load(f)\n",
    "else:\n",
    "    checkpoint_data = {}\n",
    "\n",
    "completed_indices = set(checkpoint_data.keys())\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-2-2b-it\",\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "def generate_description(artist, song, lyrics):\n",
    "    prompt = f\"\"\"Describe the following song based on the lyrics in a **comma-separated list** of adjectives and stylistic traits (can be more complex expressions or just simple words that a person would use to describe the song). \n",
    "    The description should include **mood, atmosphere, style, lyrical structure, and the artist's name**.\n",
    "    \n",
    "    Artist: {artist}\n",
    "    Song: {song}\n",
    "    Lyrics: {lyrics[:2000]}\n",
    "    \n",
    "    Description:\"\"\"\n",
    "\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "        outputs = pipe(messages, max_new_tokens=100, batch_size=32)\n",
    "        \n",
    "        return outputs[0][\"generated_text\"][-1][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating description: {e}\")\n",
    "        return None\n",
    "\n",
    "requests_made = 0\n",
    "start_time = time.time()\n",
    "\n",
    "if \"description\" not in dataset.column_names:\n",
    "    dataset = dataset.add_column(\"description\", [\"\"] * len(dataset))\n",
    "    \n",
    "new_descriptions = dataset[\"description\"]\n",
    "\n",
    "for i, desc in checkpoint_data.items():\n",
    "    new_descriptions[int(i)] = desc\n",
    "\n",
    "for i in tqdm(range(len(dataset)), desc=\"Generating descriptions\"):\n",
    "    if str(i) in completed_indices:\n",
    "        continue\n",
    "\n",
    "    artist = dataset[i][\"artist\"]\n",
    "    song = dataset[i][\"song\"]\n",
    "    lyrics = dataset[i][\"text\"]\n",
    "\n",
    "    description = generate_description(artist, song, lyrics)\n",
    "    \n",
    "    if description:\n",
    "        new_descriptions[i] = description\n",
    "        checkpoint_data[str(i)] = description\n",
    "        completed_indices.add(str(i))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "            json.dump(checkpoint_data, f)\n",
    "            \n",
    "dataset = dataset.remove_columns(\"description\")\n",
    "dataset = dataset.add_column(\"description\", new_descriptions)\n",
    "\n",
    "dataset.push_to_hub(OUTPUT_DATASET_REPO)\n",
    "\n",
    "print(\"Dataset successfully updated and pushed to Hugging Face Hub!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data : "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T18:41:55.003331Z",
     "start_time": "2025-02-05T18:41:48.296624Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "df = pd.read_parquet(\"hf://datasets/petkopetkov/spotify-million-song-dataset-descriptions/data/train-00000-of-00001.parquet\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T18:41:56.229889Z",
     "start_time": "2025-02-05T18:41:56.209800Z"
    }
   },
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \\\n",
       "0  Look at her face, it's a wonderful face  \\r\\nA...   \n",
       "1  Take it easy with me, please  \\r\\nTouch me gen...   \n",
       "2  I'll never know why I had to go  \\r\\nWhy I had...   \n",
       "3  Making somebody happy is a question of give an...   \n",
       "4  Making somebody happy is a question of give an...   \n",
       "\n",
       "                                         description  \n",
       "0  romantic, nostalgic, upbeat, sentimental, warm...  \n",
       "1  romantic, gentle, soothing, tender, intimate, ...  \n",
       "2  upbeat, optimistic, reflective, romantic, live...  \n",
       "3  upbeat, joyful, playful, optimistic, catchy, r...  \n",
       "4  uplifting, cheerful, energetic, catchy, playfu...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
       "      <td>romantic, nostalgic, upbeat, sentimental, warm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
       "      <td>romantic, gentle, soothing, tender, intimate, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\r\\nWhy I had...</td>\n",
       "      <td>upbeat, optimistic, reflective, romantic, live...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>upbeat, joyful, playful, optimistic, catchy, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>uplifting, cheerful, energetic, catchy, playfu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Finetuning a first model : __SmolLM__"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-05T18:59:53.106801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Model and dataset\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM2-135M\"  # Use small variant\n",
    "DATASET_NAME = \"petkopetkov/spotify-million-song-dataset-descriptions\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)\n",
    "\n",
    "# Define LoRA configuration (for efficient finetuning)\n",
    "lora_config = LoraConfig(\n",
    "    r=8, lora_alpha=16, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    input_texts = [f\"Prompt: {desc}\\nOutput: {text}\" for desc, text in zip(examples[\"description\"], examples[\"text\"])]\n",
    "    # Tokenize inputs\n",
    "    tokenized_inputs = tokenizer(input_texts, padding=\"max_length\", truncation=True, max_length=256)\n",
    "    # Shift labels to the right by one position for causal language modeling\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Preprocess dataset\n",
    "tokenized_datasets = train_data.map(tokenize_function, batched=True, remove_columns=[\"description\", \"text\"])\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./smollm-spotify-ft\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "trainer.push_to_hub()\n",
    "tokenizer.save_pretrained(\"./smollm-spotify-ft\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
